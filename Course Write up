First, set my working directory and read the dataset into R.<br><pre><code>setwd("D:/Coursera/PracticalMachineLearning")
training = read.csv("pml-training.csv", header = TRUE)
testing = read.csv("pml-testing.csv", header = TRUE)
dim(training)
summary(training)</code></pre>dim and summary give us a preview of how the data looks like and also help select the variables we want.<br>Aware that this experiment contains much more data than we needed, I decided to pick some variable related to the prediction we're going to made. According to the document of the experiment and common sense, movement and position variables of arm, dumbbell, belt and forearm are most related to the manner of exercise, aka classe. So those variables are selected:<br><pre><code>myvar &lt;- c("classe", grep("[xyz]$|^roll|^pitch|^yaw|^total", names(dat), value=TRUE))
part &lt;- training[,myvar]</code></pre>Since the outcome variable is a categorical one, a prediction tree is used first to see whether it works.<br><pre><code>modFit &lt;- train(classe ~ .,method="rpart",data=part)
print(modFit$finalModel)</code></pre>The result shown as below:<br><pre><code>n= 19622 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 19622 14042 A (0.28 0.19 0.17 0.16 0.18)  
   2) roll_belt&lt; 130.5 17977 12411 A (0.31 0.21 0.19 0.18 0.11)  
     4) pitch_forearm&lt; -33.95 1578    10 A (0.99 0.0063 0 0 0) *
     5) pitch_forearm&gt;=-33.95 16399 12401 A (0.24 0.23 0.21 0.2 0.12)  
      10) magnet_dumbbell_y&lt; 439.5 13870  9953 A (0.28 0.18 0.24 0.19 0.11)  
        20) roll_forearm&lt; 123.5 8643  5131 A (0.41 0.18 0.18 0.17 0.061) *
        21) roll_forearm&gt;=123.5 5227  3500 C (0.077 0.18 0.33 0.23 0.18) *
      11) magnet_dumbbell_y&gt;=439.5 2529  1243 B (0.032 0.51 0.043 0.22 0.19) *
   3) roll_belt&gt;=130.5 1645    14 E (0.0085 0 0 0 0.99) *</code></pre><br>After a simple view of the decision tree. we take it further to use random forest method. Cross validation of 10 folds is used in this random forest model.&nbsp;Predict the result with testing set and assess the prediction with out of sample error.<br><pre><code>rfFit &lt;- train(classe~ .,data=part,method="rf", trControl = trainControl(method = "cv",number = 10,repeats = 10), importance=T)
pred &lt;- predict(modFit,testing)
pred

<b>&gt; rfFit</b>
Random Forest 

19622 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
<b>Resampling: Cross-Validated (10 fold) </b>

Summary of sample sizes: 17660, 17660, 17659, 17659, 17660, 17659, ... 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
   2    0.996     0.995  0.00136      0.00173 
  27    0.995     0.994  0.00156      0.00197 
  52    0.990     0.988  0.00278      0.00352 

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 

&gt; rfFit$finalModel

Call:
 randomForest(x = x, y = y, mtry = param$mtry, importance = ..1) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 0.44%
Confusion matrix:
     A    B    C    D    E  class.error
A 5577    2    0    0    1 0.0005376344
B   14 3779    4    0    0 0.0047405847
C    0   17 3401    4    0 0.0061367621
D    0    0   38 3176    2 0.0124378109
E    0    0    0    5 3602 0.0013861935
</code></pre><div><pre></pre><pre>As reported above, the random forest model makes pretty good fit with the training data with an out-of-bag error of 0.44% and an accuracy of about 99%. We use this model to predict the manner in testing set and get the predictions as below:</pre><b>&gt; pred
 [1] B A B A A E D B A A B C B A E E A B B B</b>
Levels: A B C D E<br><br><br><br><br></div><br><br><br><br><br>
